{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto final**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objetivo**\n",
    "---\n",
    "\n",
    "Este proyecto tiene como objetivo desarrollar el análisis de una muestra de viajes de una de las empresas de servicios de transporte privado más reconocidas del mundo. Mediante dicho análisis se busca preparar una Tabla Análitica de Datos (TAD) que permita la correcta implementación de modelos que predigan el precio ideal de un viaje para los usuarios que buscan transportarse de manera segura y con conductores confiables.\n",
    "\n",
    "El dataset contiene las siguientes variables:\n",
    "\n",
    "- **key**: identificador único para cada viaje.\n",
    "\n",
    "- **fare_amount**: precio de cada viaje en dólares.\n",
    "\n",
    "- **pickup_datetime**: fecha y hora en que se activó el medidor.\n",
    "\n",
    "- **passenger_count**: el número de pasajeros en el vehículo (valor ingresado por el conductor).\n",
    "\n",
    "- **pickup_longitude**: la geolocalización (longitud) en la que se activó el medidor.\n",
    "\n",
    "- **pickup_latitude**: la geolocalización (latitud) en la que se activó el medidor.\n",
    "\n",
    "- **dropoff_longitude**: la longitud en la que se desconectó el medidor.\n",
    "\n",
    "- **dropoff_latitude**: la latitud donde se desconectó el medidor.\n",
    "\n",
    "## **Fuente**\n",
    "---\n",
    "**Datos**: Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Datos**\n",
    "---\n",
    "**Nombre:** Alan Ruiz Mondragón.  \n",
    "**Grupo:** 18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Librerias**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos librerias\n",
    "from  functools import reduce\n",
    "from  scipy.stats  import  normaltest\n",
    "from category_encoders.count import CountEncoder\n",
    "from plotly.offline import plot,iplot\n",
    "from scipy import stats\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import ksone\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from varclushi import VarClusHi\n",
    "import cufflinks as cf\n",
    "import datetime\n",
    "import emoji\n",
    "import jellyfish as jf\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import re \n",
    "import seaborn as sns\n",
    "import unicodedata\n",
    "import warnings\n",
    "import datetime as dt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import geopy.distance\n",
    "from IPython.display import Image\n",
    "import urllib\n",
    "import plotly.express as px\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "#Definimos configuraciones\n",
    "cf.go_offline()\n",
    "pd.set_option(\"display.max_columns\",200)\n",
    "pd.set_option(\"display.max_rows\",200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Funciones**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una función para limpieza de texto\n",
    "def clean_text(text, pattern=\"[^a-zA-Z0-9]\"):\n",
    "    text=str(text)\n",
    "    cleaned_text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore')\n",
    "    cleaned_text = re.sub(pattern, \" \", cleaned_text.decode(\"utf-8\"), flags=re.UNICODE)\n",
    "    cleaned_text = u' '.join(cleaned_text.lower().strip().lstrip().split())\n",
    "    return cleaned_text if cleaned_text!=\"nan\" else np.nan\n",
    "\n",
    "#Definimos una función para revisar la completitud de nuestras variables\n",
    "def completitud(df):\n",
    "    comple=pd.DataFrame(df.isnull().sum())\n",
    "    comple.reset_index(inplace=True)\n",
    "    comple=comple.rename(columns={\"index\":\"columna\",0:\"total\"})\n",
    "    comple[\"completitud\"]=(1-comple[\"total\"]/df.shape[0])*100\n",
    "    comple=comple.sort_values(by=\"completitud\",ascending=True)\n",
    "    comple.reset_index(drop=True,inplace=True)\n",
    "    return comple\n",
    "\n",
    "#Definimos funciones para revisar las variables unitarias\n",
    "def unitarias(df,col):\n",
    "    result=pd.DataFrame(df[col].value_counts(1))\n",
    "    if result.shape[0]>0:\n",
    "        if (result[col].values[0]>.91) :\n",
    "            print(f\"{col} -- VARIABLE UNITARIA\")\n",
    "\n",
    "def categoricas(df,col):\n",
    "    result=pd.DataFrame(df[col].value_counts(1))\n",
    "    if result.shape[0]>0:\n",
    "        if (result[col].values[0]>.91) :\n",
    "            print(f\"{col} -- VARIABLE UNITARIA\")\n",
    "        result[col]=result[col].map(lambda x:str(round(x*100,2))+\"%\")\n",
    "        result.reset_index(inplace=True)\n",
    "        result.columns=[col+\"_valores\",\"%_aparicion\"]\n",
    "    return result\n",
    "\n",
    "#Definimos funciones para realizar visualizaciones\n",
    "def bar(df,col,title,x_title=\"\",y_title=\"\"):\n",
    "    layout = go.Layout(font_family=\"JetBrains Mono, monospace\",\n",
    "    font_color=\"black\",title_text=title,title_font_size=30,xaxis= {\"title\": {\"text\": x_title,\"font\": {\"family\": 'JetBrains Mono, monospace',\"size\": 18,\n",
    "        \"color\": '#000000'}}},yaxis= {\"title\": {\"text\": y_title,\"font\": {\"family\": 'JetBrains Mono monospace',\"size\": 18,\n",
    "        \"color\": '#000000'}}},title_font_family=\"JetBrains Mono, monospace\",title_font_color=\"#000000\",template=\"plotly_white\")\n",
    "    aux=pd.DataFrame(df[col].value_counts()).reset_index().rename(columns={\"index\":\"conteo\"})\n",
    "    fig=aux.iplot(kind='bar',x=\"conteo\",y=col,title=title,asFigure=True,barmode=\"overlay\",sortbars=True,color=\"#000000\",layout=layout)\n",
    "    fig.update_layout(width=800)\n",
    "    fig.update_traces(marker_color='#005a96')\n",
    "    return fig\n",
    "\n",
    "def pie(df,col,title,x_title=\"\",y_title=\"\"):\n",
    "    layout = go.Layout(font_family=\"JetBrains Mono, monospace\",\n",
    "    font_color=\"black\",title_text=title,title_font_size=30,xaxis= {\"title\": {\"text\": x_title,\"font\": {\"family\": 'JetBrains Mono, monospace',\"size\": 18,\n",
    "        \"color\": '#000000'}}},yaxis= {\"title\": {\"text\": y_title,\"font\": {\"family\": 'JetBrains Mono monospace',\"size\": 18,\n",
    "        \"color\": '#000000'}}},title_font_family=\"JetBrains Mono, monospace\",title_font_color=\"#000000\",template=\"plotly_white\")\n",
    "    #layout = go.Layout(template=\"plotly_white\")\n",
    "    colors=[ \"#581845\", \"#900c3f\",\"#c70039\",\"#ff5733\",\"#ffc305\",\"#005ba3\",\"#0061a9\",\"#1567af\",\"#226cb6\",\"#2c72bc\", \"#0061a9\",\"#4c79b7\",\"#7492c6\",\"#98acd4\",\"#bbc7e2\",\"#dde3f1\",\"#ffffff\"\n",
    "]\n",
    "    aux=pd.DataFrame(df[col].value_counts()).reset_index().rename(columns={\"index\":\"conteo\"})\n",
    "    fig=aux.iplot(kind='pie',labels=\"conteo\",values=col,title=title,asFigure=True,theme=\"white\")\n",
    "    \n",
    "    fig.update_traces(textfont_size=10,\n",
    "                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))\n",
    "    fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "    fig.update_layout(font_family=\"Courier New, monospace\",\n",
    "    font_color=\"black\",title_text=title,title_font_size=30,title_font_family=\"Courier New, monospace\",title_font_color=\"#004878\",template=\"plotly_white\")\n",
    "    return fig\n",
    "    \n",
    "def box(df,col,title):\n",
    "    layout = go.Layout(font_family=\"Courier New, monospace\",\n",
    "    font_color=\"black\",title_text=title,title_font_size=30,xaxis= {\"title\": {\"font\": {\"family\": 'Courier New, monospace',\"size\": 18,\n",
    "        \"color\": '#002e4d'}}},title_font_family=\"Courier New, monospace\",title_font_color=\"#004878\",template=\"plotly_white\")\n",
    "    fig=df[[col]].iplot(kind='box',title=title,asFigure=True,theme=\"white\",layout=layout,color=\"#005a96\", boxpoints='outliers')\n",
    "    return fig\n",
    "\n",
    "def histogram(df,col,bins,title):\n",
    "    layout = go.Layout(font_family=\"Courier New, monospace\",\n",
    "    font_color=\"black\",title_text=title,title_font_size=30,xaxis= {\"title\": {\"font\": {\"family\": 'Courier New, monospace',\"size\": 18,\n",
    "        \"color\": '#002e4d'}}},title_font_family=\"Courier New, monospace\",title_font_color=\"#004878\",template=\"plotly_white\")\n",
    "    fig=df[[col]].iplot(kind='histogram',x=col,bins=bins,title=title,asFigure=True,theme=\"white\",layout=layout,color=\"#003e6c\")\n",
    "    fig.update_traces(opacity=0.90)\n",
    "    return fig\n",
    "\n",
    "#Definimos funciones para la revisión de outliers\n",
    "def OUTLIERS(df,cols):\n",
    "    results=pd.DataFrame()\n",
    "    data_iqr=df.copy()\n",
    "    data_per=df.copy()\n",
    "    total=[]\n",
    "    total_per=[]\n",
    "    total_z=[]\n",
    "    indices_=[]\n",
    "\n",
    "    for col in cols:\n",
    "        #IQR\n",
    "        Q1=df[col].quantile(0.25)\n",
    "        Q3=df[col].quantile(0.75)\n",
    "        IQR=Q3-Q1\n",
    "        INF=Q1-1.5*(IQR)\n",
    "        SUP=Q3+1.5*(IQR)\n",
    "    \n",
    "        \n",
    "        n_outliers=df[(df[col] < INF) | (df[col] > SUP)].shape[0]\n",
    "        total.append(n_outliers)\n",
    "        indices_iqr=list(df[(df[col] < INF) | (df[col] > SUP)].index)\n",
    "        #data_iqr=data_iqr[~(data_iqr[col] < INF) | (data_iqr[col] > SUP)].reset_index(drop=True)\n",
    "        \n",
    "        #Percentiles\n",
    "        INF_pe=np.percentile(df[col].dropna(),5)\n",
    "    \n",
    "        SUP_pe=np.percentile(df[col].dropna(),95)\n",
    "        n_outliers_per=df[(df[col] < INF_pe) | (df[col] > SUP_pe)].shape[0]\n",
    "        total_per.append(n_outliers_per)\n",
    "        indices_per=list(df[(df[col] < INF_pe) | (df[col] > SUP_pe)].index)\n",
    "        #data_per=data_per[~(data_per[col] < INF_pe) | (data_per[col] > SUP_pe)].reset_index(drop=True)\n",
    "        \n",
    "        #MEAN CHANGE\n",
    "        \n",
    "        #Obtenemos todos los percentiles además del máximo\n",
    "        perc_100 = [x / 100 for x in range(100)]\n",
    "        dist = df[col].describe(perc_100).iloc[4:]\n",
    "        #Obtenemos el cambio entre percentiles\n",
    "        change_dist = df[col].describe(perc_100).iloc[4:].diff()\n",
    "        #Obtenemos el cambio promedio entre percentiles\n",
    "        mean_change = df[col].describe(\n",
    "            perc_100).iloc[4:].diff().mean()\n",
    "        #Si el cambio entre el percentil 99 y el maximo es mayor a el cambio promedio entonces:\n",
    "        if change_dist[\"max\"] > mean_change:\n",
    "            #La banda superior será el máximo menos el cambio promedio\n",
    "            ub = dist[\"max\"] - mean_change\n",
    "            #si la banda superior es más pequeña que el percentil 99 , modificamos la banda para que tome el percentil 99\n",
    "            if ub < dist[\"99%\"]:\n",
    "                ub = dist[\"99%\"]\n",
    "        else:\n",
    "        #Si el cambio entre el percentil 99 y el maximo es menor o igual a el cambio promedio entonces se toma el percentil 99\n",
    "            ub = dist[\"max\"]\n",
    "\n",
    "        if change_dist[\"1%\"] > mean_change:\n",
    "            lb = dist[\"0%\"] + mean_change\n",
    "            if lb > dist[\"1%\"]:\n",
    "                lb = dist[\"1%\"]\n",
    "        else:\n",
    "            lb = dist[\"0%\"]\n",
    "        n_total_z=df[(df[col] < lb) | (df[col] > ub)].shape[0]\n",
    "        total_z.append(n_total_z)\n",
    "        indices_z=list(df[(df[col] < lb) | (df[col] > ub)].index)\n",
    "        \n",
    "        indices_.append(aux_outliers(indices_iqr,indices_per,indices_z))\n",
    "\n",
    "    results[\"features\"]=cols\n",
    "    results[\"n_outliers_IQR\"]=total\n",
    "    results[\"n_outliers_Percentil\"]=total_per\n",
    "    results[\"n_outliers_Mean_Change\"]=total_z\n",
    "    results[\"n_outliers_IQR_%\"]=round((results[\"n_outliers_IQR\"]/df.shape[0])*100,2)\n",
    "    results[\"n_outliers_Percentil_%\"]=round((results[\"n_outliers_Percentil\"]/df.shape[0])*100,2)\n",
    "    results[\"n_outliers_Mean_Change_%\"]=round((results[\"n_outliers_Mean_Change\"]/df.shape[0])*100,2)\n",
    "    results[\"indices\"]=indices_\n",
    "    results[\"total_outliers\"]=results[\"indices\"].map(lambda x:len(x))\n",
    "    results[\"%_outliers\"]=results[\"indices\"].map(lambda x:round(((len(x)/df.shape[0])*100),2))\n",
    "    results=results[['features', 'n_outliers_IQR', 'n_outliers_Percentil',\n",
    "       'n_outliers_Mean_Change', 'n_outliers_IQR_%', 'n_outliers_Percentil_%',\n",
    "       'n_outliers_Mean_Change_%',  'total_outliers', '%_outliers','indices']]\n",
    "    return results\n",
    "    \n",
    "def aux_outliers(a,b,c):\n",
    "    a=set(a)\n",
    "    b=set(b)\n",
    "    c=set(c)\n",
    "    \n",
    "    a_=a.intersection(b)\n",
    "\n",
    "    b_=b.intersection(c)\n",
    "\n",
    "    c_=a.intersection(c)\n",
    "\n",
    "    outliers_index=list(set(list(a_)+list(b_)+list(c_)))\n",
    "    return outliers_index\n",
    "\n",
    "#Definimos funciones para la revisión de outliers el método a utilizar\n",
    "def chi_square(df,col,valor_miss):\n",
    "    x_i=df[col].fillna(valor_miss).value_counts()\n",
    "    k=x_i.sum()\n",
    "    p_i=df[col].dropna().value_counts(1)\n",
    "    m_i=k*p_i\n",
    "    print(x_i)\n",
    "    print(m_i)\n",
    "    chi=chisquare(f_obs=x_i,f_exp=m_i)\n",
    "    p_val=chi.pvalue\n",
    "    alpha=0.05\n",
    "    if p_val<alpha:\n",
    "        print(\"Rechazamos HO (La porporción de categorias es la misma que la general)\")\n",
    "    else:\n",
    "        print(\"Aceptamos HO (La porporción de categorias es la misma que la general)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los datos\n",
    "df = pd.read_csv('/Users/alanruizmondragon/Documents/PERSONAL/Code/Diplomados/Ciencia de datos/Modulo I/proyecto_final/data/uber.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizamos el dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna desconocida y la variable key debido a que esta variable corresponde alos mismos datos de la variable pickup_datetime\n",
    "df.drop(columns={'Unnamed: 0','key'}, axis=1, inplace=True)\n",
    "#Visualizamos nuevamente el dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modificamos la variable de fecha a un tipo datetime para su posterior manipulación\n",
    "df.pickup_datetime = pd.to_datetime(df.pickup_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizamos la información inicial del dataset dataset\n",
    "print('Visualizamos el dataset')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tamaño**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizamos el tamaño del dataset\n",
    "print('El dataset contienene {} columnas y {} registros.'.format(df.shape[1], \"{:,}\".format(int(df.shape[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Información**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos información del dataset\n",
    "print('Obtenemos la información del dataset\\n')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizamos el número de columnas por tipo\n",
    "df.dtypes.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diccionario**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos el diccionario de datos del dataset principal\n",
    "diccionario = pd.read_excel('/Users/alanruizmondragon/Documents/PERSONAL/Code/Diplomados/Ciencia de datos/Modulo I/proyecto_final/diccionario/diccionario.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizamos el diccionario de datos del dataset principal \n",
    "diccionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Calidad de datos**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición\n",
    "\n",
    "La calidad de datos se refiere al grado en que los datos se ajustan a los criterios establecidos para su uso. Estos criterios de calidad de los datos abarcan aspectos como exactitud, coherencia, actualización, exhaustividad y decirigibilidad. La calidad de los datos es uno de los principales factores en el éxito de la base de datos. Si los datos no cumplen con los criterios de calidad adecuados, la información generada a partir de dichos datos también será maliciosa o inútil. Por lo tanto, es crítica la necesidad de vigilancia y mejora de la calidad de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el dataset original antes de aplicar la calidad de datos\n",
    "original_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etiquetado de variables\n",
    "\n",
    "En este aparatdo se realizará la revisión del dataset y se etiquetarán las variables para identificarlas a lo largo dle tratamiento de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contenido\n",
    "\n",
    "Visualizamos el contenido y las columnas de los datasets nuevamente para registrar los siguientes pasos de calidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizamos el dataset\n",
    "display(df.head())\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizamos las columnas contenidas dentro del dataset principal\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizamos el contenido del datset principal\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizamos la distribución del contenido por tipos de datos de la columna del dataset principal \n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribución del contenido\n",
    "\n",
    "Los tipos de datos contenidos en el dataset se distribuyen de la siguiente manera\n",
    "\n",
    "- Columnas de tipo float64: 5\n",
    "- Columnas de tipo int64: 1\n",
    "- Columnas de tipo object: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prefijos\n",
    "\n",
    "Realizamos el etiquetado de las variables utilizando los siguientes prefijos que utilizaremos para los diferentes tipos de variables:\n",
    "\n",
    "- **c_**: Variables numericas (discretas y continuas).\n",
    "- **v_**: Variables categoricas.\n",
    "- **d_**: Variables tipo fecha.\n",
    "- **t_**: Variables de texto (comentarios, descripciones, url, etc.).\n",
    "- **g_**: Variables geograficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etiquetado\n",
    "\n",
    "**Nota**: Es importante mencionar que la variable target es la variable *****fare_amount*****, ya que el objetivo posterior al análisis es realizar un modelo de aprendizaje supervisado que nos permita predecir el precio de un viaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos el etiquetado de las variables del datset principal de acuerdo a los prefijos anteriormente establecidos\n",
    "c_feats=[\"fare_amount\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "v_feats=[\"passenger_count\"] \n",
    "d_feats=[\"pickup_datetime\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos \n",
    "c_feats_new = [\"c_\" + x for x in c_feats]\n",
    "v_feats_new = [\"v_\" + x for x in v_feats]\n",
    "d_feats_new = [\"d_\" + x for x in d_feats]\n",
    "\n",
    "#Renombramos las columnas\n",
    "df.rename(columns = dict(zip(c_feats, c_feats_new)), inplace = True)\n",
    "df.rename(columns = dict(zip(v_feats, v_feats_new)), inplace = True)\n",
    "df.rename(columns = dict(zip(d_feats, d_feats_new)), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validamos el etiquetado\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completitud\n",
    "\n",
    "Se recomienda que las variables cuenten con una completitud de al menos 80%, de modo que las que no cumplan con esta condición serán eliminadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revisamos la completitud de las columnas del dataset y observamos que aunque no hay peridda de valores menor al 80%, ahora tenemos una variables con 709 valores faltantes\n",
    "completitud(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos los duplicados\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistencia\n",
    "\n",
    "En este apartado se realizará la revisión de la naturaleza de las variable, así como algunas modificaciones al dataset inicial que nos será de ayuda para el tratamiento posterior de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revisamos las variables categoricas con la siguiente iteración donde se filtran los valores de las variable de forma única y en un formato de cadena\n",
    "for i in df.filter(like=\"v_\"):\n",
    "  print(i)\n",
    "  values = df[i].astype(str).unique()\n",
    "  values.sort()\n",
    "  display(values)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validación\n",
    "\n",
    "Después de visualizar de forma general las variables categóricas, se revisará cada una a detalle y se harán los arreglos pertinentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c_passenger_count**\n",
    "Observamos que hay valores fuera de la naturaleza de las variables, pues no puede haber un servicio de taxi de 0 pasajeros o 208."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTotal de datos inconsistentes para la variable \\033[1mc_passenger_count\\033[0m:')\n",
    "df[(df['v_passenger_count'] == 0) | (df['v_passenger_count'] == 208)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el porcentaje que estos datos invalidos reresentan\n",
    "print('\\nPorcentaje de datos inconsistentes para la variable \\033[1mc_passenger_count\\033[0m:')\n",
    "(df[(df['v_passenger_count'] == 0) | (df['v_passenger_count'] == 208)].shape[0]/df.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manipulación de valores\n",
    "Al revisar las variables continuas se pueden observar las siguientes variables con inconsistencias y sus respectivas causas:\n",
    "\n",
    "1. **c_passenger_count**: Esta variable representa el número de pasajeros de cada viaje, normalmente los viajes son en autos con máximo 4 lugares y coches mas grandes con hasta 6 lugares, pero hay datos que se encuentran fuera de la naturaleza de la variable; para el caso de esta variable, el porcentaje de datos inválidos es muy pequeño, **0.3550%**, de modo que las eliminamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observamos el tamaño del df\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos los datos invalidos de la variable c_passenger_count\n",
    "df = df[(df['v_passenger_count'] != 0) & (df['v_passenger_count'] != 208)]\n",
    "\n",
    "#Reseteamos el índice\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el nuevo tamaño del dataframe\n",
    "print('El nuevo dataset contienene {} columnas y {} registros.'.format(df.shape[1], \"{:,}\"\n",
    "        .format(int(df.shape[0]))))\n",
    "\n",
    "#Obtenemos el porcentaje de datos perdidos sobre el dataset original\n",
    "print('El porcentaje de datos perdidos es: {}.'.format((1-df.shape[0]/original_df.shape[0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revisamos las variables categoricas con la siguiente iteración donde se filtran los valores de las variable de forma única y en un formato de cadena\n",
    "for i in df.filter(like=\"c_\"):\n",
    "  print(i)\n",
    "  values = df[i].astype(str).unique()\n",
    "  values.sort()\n",
    "  display(values)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validación\n",
    "\n",
    "Después de visualizar de forma general las variables continuas, se revisará cada una a detalle y se harán los arreglos pertinentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c_fare_amount**\n",
    "\n",
    "Observamos que hay valores negativos en el precio de los viajes, esto significa que hay valores fuera de la naturaleza de la variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos la cantidad de registros con precios negativos\n",
    "print('\\nTotal de datos inconsistentes para la variable \\033[1mc_fare_amount\\033[0m:')\n",
    "df[df['c_fare_amount'] < 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el porcentaje que estos datos invalidos reresentan\n",
    "print('\\nPorcentaje de datos inconsistentes para la variable \\033[1mc_fare_amount\\033[0m:')\n",
    "(df[df['c_fare_amount'] < 0].shape[0]/df.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pickup_latitude**, **pickup_longitude**, **dropoff_latitude**, **dropoff_longitude**\n",
    "\n",
    "Observamos que hay valores que no se adecuan a la naturaleza de las variables, pues al ser coordenadas se pueden validar con la meición correcta sel sistema de medición de coornedadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizamos laa medición correcta de los grados de la tierra con los que tendría que ser válidos en el dataset\n",
    "Image(url='https://upload.wikimedia.org/wikipedia/commons/thumb/5/58/Latitud_y_Longitud_en_la_Tierra.svg/2880px-Latitud_y_Longitud_en_la_Tierra.svg.png', width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el rango de latituds y longitudes que debería seguir el dataset de acuerdo a la medición de grados de la tierra\n",
    "print(\"\\nValidamos para las variables con \\033[1mlatitudes\\033[0m\\n\")\n",
    "display(df[(df.c_pickup_latitude>90)])\n",
    "display(df[(df.c_dropoff_latitude>90)])\n",
    "display(df[(df.c_pickup_latitude<-90)])\n",
    "display(df[(df.c_dropoff_latitude<-90)])\n",
    "\n",
    "#Obtenemos el total de datos invalidos\n",
    "print('\\nTotal de datos inconsistentes para las variables con \\033[1mlatitudes\\033[0m:')\n",
    "display((df[(df.c_pickup_latitude>90)].shape[0] + df[(df.c_dropoff_latitude>90)].shape[0] + \n",
    "df[(df.c_pickup_latitude<-90)].shape[0] + df[(df.c_dropoff_latitude<-90)].shape[0]))\n",
    "\n",
    "#Obtenemos el porcentaje que estos datos invalidos reresentan\n",
    "print('\\nPorcentaje de datos inconsistentes para las variables con \\033[1mlatitudes\\033[0m:')\n",
    "((df[(df.c_pickup_latitude>90)].shape[0] + df[(df.c_dropoff_latitude>90)].shape[0] + \n",
    "df[(df.c_pickup_latitude<-90)].shape[0] + df[(df.c_dropoff_latitude<-90)].shape[0])/df.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el rango de latituds y longitudes que debería seguir el dataset de acuerdo a la medición de grados de la tierra\n",
    "print(\"\\nValidamos para las variables con \\033[1mlongitudes\\033[0m\\n\")\n",
    "display(df[(df.c_pickup_longitude>180)])\n",
    "display(df[(df.c_dropoff_longitude>180)])\n",
    "display(df[(df.c_pickup_longitude<-180)])\n",
    "display(df[(df.c_dropoff_longitude<-180)])\n",
    "\n",
    "#Obtenemos el total de datos invalidos\n",
    "print('\\nPorcentaje de datos inconsistentes para las variables con \\033[1mlatitudes\\033[0m:')\n",
    "display((df[(df.c_pickup_longitude>180)].shape[0] + df[(df.c_dropoff_longitude>180)].shape[0] + \n",
    "df[(df.c_pickup_longitude<-180)].shape[0] + df[(df.c_dropoff_longitude<-180)].shape[0]))\n",
    "\n",
    "#Obtenemos el porcentaje que estos datos invalidos reresentan\n",
    "print('\\nPorcentaje de datos inconsistentes para las variables con \\033[1mlatitudes\\033[0m:')\n",
    "((df[(df.c_pickup_longitude>180)].shape[0] + df[(df.c_dropoff_longitude>180)].shape[0] + \n",
    "df[(df.c_pickup_longitude<-180)].shape[0] + df[(df.c_dropoff_longitude<-180)].shape[0])/df.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el porcentaje total de los datos inconsistentes para las variables con coordenadas\n",
    "print('\\nPorcentaje de datos inconsistentes para las variables con \\033[1mcoordenadas\\033[0m:')\n",
    "((df[(df.c_pickup_latitude>90)].shape[0] + df[(df.c_dropoff_latitude>90)].shape[0] + \n",
    "df[(df.c_pickup_latitude<-90)].shape[0] + df[(df.c_dropoff_latitude<-90)].shape[0])/df.shape[0]) * 100 + ((df[(df.c_pickup_longitude>180)].shape[0] + df[(df.c_dropoff_longitude>180)].shape[0] + \n",
    "df[(df.c_pickup_longitude<-180)].shape[0] + df[(df.c_dropoff_longitude<-180)].shape[0])/df.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manipulación de valores\n",
    "Al revisar las variables continuas se pueden observar las siguientes variables con inconsistencias y sus respectivas causas:\n",
    "\n",
    "1. **c_fare_amount**: Esta variable representa el número de pasajeros de cada viaje, normalmente los viajes son en autos con máximo 4 lugares y coches mas grandes con hasta 6 lugares, pero hay datos que se encuentran fuera de la naturaleza de la variables; para el caso de esta variable, el porcentaje de datos inválidos es muy pequeño, **0.0085%**, de modo que las eliminamos.\n",
    "\n",
    "2. **pickup_latitude**, **pickup_longitude**, **dropoff_latitude**, **dropoff_longitude**: Las variables que muestran la geolocalización tienen valores fuera de su naturaleza, pues basado en la medición de coordenadas la longitud solo puede medirse entre -180° y 180°, mientras que la latitud entre -90° y 90°; para el caso de estas variable, el porcentaje de datos inválidos es muy pequeño, **0.0085%**, de modo que las eliminamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observamos el tamaño del df\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos los datos invalidos de la variable c_fare_amount\n",
    "df = df[df['c_fare_amount'] > 0]\n",
    "\n",
    "#Eliminamos los datos invalidos de las variables con coordenadas\n",
    "df = df[(df.c_pickup_latitude<90) & (df.c_dropoff_latitude<90) &\n",
    "        (df.c_pickup_latitude>-90) & (df.c_dropoff_latitude>-90) &\n",
    "        (df.c_pickup_longitude<180) & (df.c_dropoff_longitude<180) &\n",
    "        (df.c_pickup_longitude>-180) & (df.c_dropoff_longitude>-180)]\n",
    "\n",
    "#Reseteamos el índice\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el nuevo tamaño del dataframe\n",
    "print('El nuevo dataset contienene {} columnas y {} registros.'.format(df.shape[1], \"{:,}\"\n",
    "        .format(int(df.shape[0]))))\n",
    "\n",
    "#Obtenemos el porcentaje de datos perdidos sobre el dataset original\n",
    "print('El porcentaje de datos perdidos es: {}.'.format((1-df.shape[0]/original_df.shape[0])*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['d_pickup_datetime'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos nuevas columnas que nos podrán ayudar al tratamiento de los datos\n",
    "df['v_year'] = df.d_pickup_datetime.dt.year\n",
    "df['v_month'] = df.d_pickup_datetime.dt.month\n",
    "df['v_weekday'] = df.d_pickup_datetime.dt.weekday\n",
    "df['v_hour'] = df.d_pickup_datetime.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos categorías segmentadas de trimestre y horarios de los viajes\n",
    "df['v_quarter'] = df.v_month.map({1:'Q1',2:'Q1',3:'Q1',4:'Q2',5:'Q2',6:'Q2',7:'Q3',\n",
    "                                      8:'Q3',9:'Q3',10:'Q4',11:'Q4',12:'Q4'})\n",
    "\n",
    "df['v_hourly_segment'] = df.v_hour.map({0:'H1',1:'H1',2:'H1',3:'H1',4:'H2',5:'H2',6:'H2',7:'H2',8:'H3',\n",
    "                                     9:'H3',10:'H3',11:'H3',12:'H4',13:'H4',14:'H4',15:'H4',16:'H5',\n",
    "                                     17:'H5',18:'H5',19:'H5',20:'H6',21:'H6',22:'H6',23:'H6'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos la distancia de los viajes\n",
    "df['c_distance']=[round(geopy.distance.distance((df.c_pickup_latitude[i], df.c_pickup_longitude[i]),(df.c_dropoff_latitude[i], df.c_dropoff_longitude[i])).km,2) for i in df.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manipulación de valores\n",
    "Al revisar las variables continuas se pueden observar las siguientes variables con inconsistencias y sus respectivas causas:\n",
    "\n",
    "1. **d_pickup_datetime**: Debido a que esta variable ha sido dividia en varias columnas que nos dan información más concreta, procedemos a eliminarla.\n",
    "\n",
    "2. **v_month**: La variable de mes será removida ya que hemos creado una categoría que representa el trimestre.\n",
    "\n",
    "3. **v_hour**: La variable de hora será removida ya que hemos creado una categoría que representa la hora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna de pickup_datetime, month, hour, pues ahora tenemos mejor distribuida la información con las columnas generadas\n",
    "df.drop(['d_pickup_datetime','v_month', 'v_hour',], axis=1, inplace=True)\n",
    "\n",
    "#Reseteamos el índice\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizamos el nuevo dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el nuevo tamaño del dataframe\n",
    "print('El nuevo dataset contienene {} columnas y {} registros.'.format(df.shape[1], \"{:,}\"\n",
    "        .format(int(df.shape[0]))))\n",
    "\n",
    "#Obtenemos el porcentaje de datos perdidos sobre el dataset original\n",
    "print('El porcentaje de datos perdidos es: {}.'.format((1-df.shape[0]/original_df.shape[0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completitud\n",
    "\n",
    "Se recomienda que las variables cuenten con una completitud de al menos 80%, de modo que las que no cumplan con esta condición serán eliminadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revisamos la completitud de las columnas del dataset y observamos que aunque no hay peridda de valores menor al 80%, ahora tenemos una variables con 709 valores faltantes\n",
    "completitud(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total de registros duplicados de forma general\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPorcentaje de datos \\033[1mduplicados\\033[0m:')\n",
    "(df.duplicated().sum()/df.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos los duplicados al representar un porcentaje bajo de registros y mantenemos el primer elemento de los duplicados\n",
    "df.drop_duplicates(keep = 'first', inplace = True)\n",
    "\n",
    "#Reseteamos el índice\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validamos la eliminacipon de duplicados\n",
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el nuevo tamaño del dataframe\n",
    "print('El nuevo dataset contienene {} columnas y {} registros.'.format(df.shape[1], \"{:,}\"\n",
    "        .format(int(df.shape[0]))))\n",
    "\n",
    "#Obtenemos el porcentaje de datos perdidos sobre el dataset original\n",
    "print('El porcentaje de datos perdidos es: {}.'.format((1-df.shape[0]/original_df.shape[0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observaciones\n",
    "\n",
    "En esta ocasión, en el presente dataset, ninguna de las columnas contó con una completitud menor al umbral definido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis (EDA)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observamos el dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos la información\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el tamaño del dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambiamos los tipos de datos para una mejor manipulación\n",
    "df[\"v_passenger_count\"] = df[\"v_passenger_count\"].astype(\"string\")\n",
    "df[\"v_year\"] = df[\"v_year\"].astype(\"string\")\n",
    "df[\"v_weekday\"] = df[\"v_weekday\"].astype(\"string\")\n",
    "df[\"v_hourly_segment\"] = df[\"v_hourly_segment\"].astype(\"string\")\n",
    "df[\"v_quarter\"] = df[\"v_quarter\"].astype(\"string\")\n",
    "\n",
    "#Añadimos una columna que nos ayudará con los gráficos\n",
    "df[\"count\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable target\n",
    "fig = px.histogram(df[\"c_fare_amount\"], nbins=15, title='Gráfico de caja e histograma de la variable target: c_fare_amount',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_fare_amount')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"c_fare_amount\"].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusiones\n",
    "\n",
    "**Gráfico de cajas**: Podemos ver los outliers, al menos el valor máximo (499 USD) puede observarse bastante alejado del resto en el gráfico de cajas. Este así como el resto deberán eliminarse más adelante.\n",
    "\n",
    "**Histograma**: Podemos observar que la distribución no parece comportarse como una normal.\n",
    "\n",
    "**Estadísticos**\n",
    "\n",
    "**Promedio**: Obtuvimos un promedio de 11.3714 USD para el precio de los viajes.  \n",
    "\n",
    "**Desviación estandar**: La variablidad de los datos con respecto al promedio es de 9.9081.  \n",
    "\n",
    "**Mínimo**: El valor mínimo de un viaje es 0.0100 centavos de USD.  \n",
    "\n",
    "**Máximo** 499.0000.  \n",
    "\n",
    "**Cuartiles**  \n",
    "\n",
    "**25%** -> 6.0000  \n",
    "**50%** -> 8.5000  \n",
    "**75%** -> 12.5000  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizamos descriptivos \n",
    "df[[\"c_pickup_longitude\", \n",
    "    \"c_pickup_latitude\", \n",
    "    \"c_dropoff_longitude\",\n",
    "    \"c_dropoff_latitude\", \n",
    "    \"c_distance\"]].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables de recolección del pasaje\n",
    "\n",
    "**c_pickup_longitude**, **c_pickup_latitude**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable target\n",
    "fig = px.histogram(df[\"c_pickup_longitude\"], nbins=15, title='Gráfico de caja e histograma de la variable c_pickup_longitude',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_pickup_longitude')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable target\n",
    "fig = px.histogram(df[\"c_pickup_latitude\"], nbins=15, title='Gráfico de caja e histograma de la variable c_pickup_latitude',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_pickup_latitude')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráficamos los lugares de recolección en el mundo que contiene el dataset\n",
    "fig = px.scatter_geo(df,\n",
    "                    lat=df.c_pickup_latitude,\n",
    "                    lon=df.c_pickup_longitude)\n",
    "fig.update_layout(\n",
    "    title = \"Países de recolección de pasaje\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráficamos los lugares de recolección en el país que cuenta con más registros en el dataset (EE.UUU)\n",
    "fig = px.scatter_geo(df,\n",
    "                    lat=df.c_pickup_latitude,\n",
    "                    lon=df.c_pickup_longitude)\n",
    "fig.update_layout(\n",
    "    title = \"Lugares de recolección de pasaje en EE.UU<br>(El país con más registros en el dataset)\",\n",
    "    geo_scope = \"usa\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables de desocupación del pasaje\n",
    "\n",
    "**c_dropoff_longitude**, **c_dropoff_latitude**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable target\n",
    "fig = px.histogram(df[\"c_dropoff_longitude\"], nbins=15, title='Gráfico de caja e histograma de la variable c_dropoff_longitude',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_dropoff_longitude')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable target\n",
    "fig = px.histogram(df[\"c_dropoff_latitude\"], nbins=15, title='Gráfico de caja e histograma de la variable c_dropoff_latitude',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_dropoff_latitude')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráficamos los lugares de recolección en el mundo que contiene el dataset\n",
    "fig = px.scatter_geo(df,\n",
    "                    lat=df.c_dropoff_latitude,\n",
    "                    lon=df.c_dropoff_longitude)\n",
    "fig.update_layout(\n",
    "    title = \"Países de desocupación de pasaje\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráficamos los lugares de recolección en el mundo que contiene el dataset\n",
    "fig = px.scatter_geo(df,\n",
    "                    lat=df.c_dropoff_latitude,\n",
    "                    lon=df.c_dropoff_longitude)\n",
    "fig.update_layout(\n",
    "    title =\"Lugares de recolección de pasaje en EE.UU<br>(El país con más registros en el dataset)\"\n",
    "    geo_scope = \"usa\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c_distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable target\n",
    "fig = px.histogram(df[\"c_distance\"], nbins=15, title='Gráfico de caja e histograma de la variable c_distance',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_distance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusiones\n",
    "\n",
    "**Gráficos espacial**: Podemos observar que la distribución de los puntos de recolección y desocupación del pasaje están centrados en EE.UU., y hay algunos puntos extraños en el mar y en la Antártida, valores que esperamos eliminar más adelante.\n",
    "\n",
    "**Gráfico de cajas**: Para el caso de la variable distancia, podemos observar la presencia de outliers, siendo el mayor oulier el máximo de distancia de viaje, el cual esta representado por **8,783.59** KM.\n",
    "\n",
    "**Histograma**: Podemos observar que la distribución no parece comportarse como una normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos las variables categóricas del dataset\n",
    "for i in df.filter(like=\"v_\"):\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**v_passenger_count**\n",
    "\n",
    "Podemos observar que norlmalmente los viajes son de **1** pasajero, representando poco más de **138K** viajes, lo que represento el **69.4%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(pd.DataFrame(df.groupby([\"v_passenger_count\"])[\"count\"].count()).reset_index(), x = 'v_passenger_count', y=\"count\", title=\"Frecuencia por tipo de viajes por número de pasajeros\", color =\"v_passenger_count\")\n",
    "fig.update_layout(yaxis_title='Count', xaxis_title='c_fare_amount')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values='count', names='v_passenger_count', title='Porcentaje de tipo de viajes por número de pasajeros')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**v_year**\n",
    "\n",
    "Podemos observar que el año con más viajes registrados es **2012**, respresentando poco más de **30K** viajes, lo que representó el **16.1%** del total de viajes registrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(pd.DataFrame(df.groupby([\"v_year\"])[\"count\"].count()).reset_index(), x = 'v_year', y=\"count\", title=\"Frecuencia de viajes por año\", color =\"v_year\")\n",
    "fig.update_layout(yaxis_title='Count', xaxis_title='v_year')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values='count', names='v_year', title='Porcentaje de viajes por año')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**v_weekday**\n",
    "\n",
    "Podemos observar que el día con más viajes registrados es el **viernes**, respresentando poco más de **30K** viajes, lo que representó el **16.2%** del total de viajes registrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(pd.DataFrame(df.groupby([\"v_weekday\"])[\"count\"].count()).reset_index(), x = 'v_weekday', y=\"count\", title=\"Frecuencia de viajes por día\", color =\"v_weekday\")\n",
    "fig.update_layout(yaxis_title='Count', xaxis_title='v_weekday')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values='count', names='v_weekday', title=\"Porcentaje de viajes por día\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**v_quarter**\n",
    "\n",
    "Podemos observar que el trimestre con más viajes registrados es el **Q2**, respresentando poco más de **55K** viajes, lo que representó el **27.6%** del total de viajes registrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(pd.DataFrame(df.groupby([\"v_quarter\"])[\"count\"].count()).reset_index(), x = 'v_quarter', y=\"count\", title=\"Frecuencia de viajes por trimestre\", color =\"v_quarter\")\n",
    "fig.update_layout(yaxis_title='Count', xaxis_title='v_quarter')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values='count', names='v_quarter', title=\"Porcentaje de viajes por trimestre\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**v_quarter**\n",
    "\n",
    "Podemos observar que el segmento de horas con más viajes registrados es el **H6**, es decir, entre las 20:00 y 23:00 horas, respresentando poco más de **43K** viajes, lo que representó el **22%** del total de viajes registrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(pd.DataFrame(df.groupby([\"v_hourly_segment\"])[\"count\"].count()).reset_index(), x = 'v_hourly_segment', y=\"count\", title=\"Frecuencia de viajes por segmento de hora\", color =\"v_hourly_segment\")\n",
    "fig.update_layout(yaxis_title='Count', xaxis_title='v_hourly_segment')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values='count', names='v_hourly_segment', title=\"Porcentaje de viajes por segmento de hora\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Datos anómalos**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogramas antes de la remoción de outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable target\n",
    "fig = px.histogram(df[\"c_fare_amount\"], nbins=15, title='Gráfico de caja e histograma de la variable target: c_fare_amount',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_fare_amount')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables de recolección del pasaje\n",
    "\n",
    "**c_pickup_longitude**, **c_pickup_latitude**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable c_pickup_longitude\n",
    "fig = px.histogram(df[\"c_pickup_longitude\"], nbins=15, title='Gráfico de caja e histograma de la variable c_pickup_longitude',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_pickup_longitude')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable c_pickup_latitude\n",
    "fig = px.histogram(df[\"c_pickup_latitude\"], nbins=15, title='Gráfico de caja e histograma de la variable c_pickup_latitude',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_pickup_latitude')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables de desocupación del pasaje\n",
    "\n",
    "**c_dropoff_longitude**, **c_dropoff_latitude**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable c_dropoff_longitude\n",
    "fig = px.histogram(df[\"c_dropoff_longitude\"], nbins=15, title='Gráfico de caja e histograma de la variable c_dropoff_longitude',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_dropoff_longitude')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable c_dropoff_latitude\n",
    "fig = px.histogram(df[\"c_dropoff_latitude\"], nbins=15, title='Gráfico de caja e histograma de la variable c_dropoff_latitude',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_dropoff_latitude')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **c_distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable c_distance\n",
    "fig = px.histogram(df[\"c_distance\"], nbins=15, title='Gráfico de caja e histograma de la variable c_distance',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_distance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación de outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Método**\n",
    "\n",
    "Debido a la naturaleza de nuestras variables, las cuales no siguen un distribución normal, utilizaremos el método univariado de percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos la función de OUTLIERS la cual muestra un resuen de los diversos métodos, debido a las pruebas realizadas anteriormente, solo mostraremos atención en la prueba de los percentiles\n",
    "outliers=OUTLIERS(df,list(df.filter(like=\"c_\")))\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos en una lista los indices identificados como atípicos\n",
    "indices = list(outliers[\"indices\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos los indicies que nos interesan\n",
    "indices = list(set(reduce(lambda x,y: x+y, indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el porcentaje de elementos que eliminaremos\n",
    "(len(indices)/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos los valores a imputar\n",
    "df[df.index.isin(indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el tamaño del actual dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.index.isin(indices)].reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el nuevo tamaño del dataframe\n",
    "print('El nuevo dataset contienene {} columnas y {} registros.'.format(df.shape[1], \"{:,}\"\n",
    "        .format(int(df.shape[0]))))\n",
    "\n",
    "#Obtenemos el porcentaje de datos perdidos sobre el dataset original\n",
    "print('El porcentaje de datos perdidos es: {}.'.format((1-df.shape[0]/original_df.shape[0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogramas antes de la remoción de outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable target\n",
    "fig = px.histogram(df[\"c_fare_amount\"], nbins=15, title='Gráfico de caja e histograma de la variable target: c_fare_amount',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_fare_amount')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables de recolección del pasaje\n",
    "\n",
    "**c_pickup_longitude**, **c_pickup_latitude**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable c_pickup_longitude\n",
    "fig = px.histogram(df[\"c_pickup_longitude\"], nbins=15, title='Gráfico de caja e histograma de la variable c_pickup_longitude',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_pickup_longitude')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable c_pickup_latitude\n",
    "fig = px.histogram(df[\"c_pickup_latitude\"], nbins=15, title='Gráfico de caja e histograma de la variable c_pickup_latitude',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_pickup_latitude')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables de desocupación del pasaje\n",
    "\n",
    "**c_dropoff_longitude**, **c_dropoff_latitude**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable c_dropoff_longitude\n",
    "fig = px.histogram(df[\"c_dropoff_longitude\"], nbins=15, title='Gráfico de caja e histograma de la variable c_dropoff_longitude',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_dropoff_longitude')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable c_dropoff_latitude\n",
    "fig = px.histogram(df[\"c_dropoff_latitude\"], nbins=15, title='Gráfico de caja e histograma de la variable c_dropoff_latitude',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_dropoff_latitude')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **c_distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos un gráfico de cajas y un histograma para revisar a la variable c_distance\n",
    "fig = px.histogram(df[\"c_distance\"], nbins=15, title='Gráfico de caja e histograma de la variable c_distance',marginal=\"box\")\n",
    "fig.update_layout(yaxis_title='count', xaxis_title='c_distance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Datos faltantes**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revisamos nuevamente los datos faltantes\n",
    "completitud(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ingenieria de variables**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consideraciones\n",
    "\n",
    "Debido a la naturaleza de las variables **continuas** las cuales son:\n",
    "\n",
    "- **c_fare_amount**: Variable de precio objetivo.\n",
    "\n",
    "- **c_pickup_longitude**: Variable de geolocalización.\n",
    "\n",
    "- **c_pickup_latitude**: Variable de geolocalización.\n",
    "\n",
    "- **c_dropoff_longitude**: Variable de geolocalización.\n",
    "\n",
    "- **c_dropoff_latitude**: Variable de geolocalización.\n",
    "\n",
    "- **c_distance**: Distancia en KM.\n",
    "\n",
    "Considero que no es no necesario aplicar ningún método de ingenieria de variables, no es así para el caso de las varibles **categóricas**, las cuales son:\n",
    "\n",
    "- **v_passenger_count**\n",
    "\n",
    "- **v_year**\n",
    "\n",
    "- **v_weekday**\n",
    "\n",
    "- **v_quarter**\n",
    "\n",
    "- **v_hourly_segment**\n",
    "\n",
    "En estas últimas aplicaremos ingeniería de variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos la creación de las variables dummy\n",
    "df = pd.get_dummies(df,columns=['v_passenger_count',\"v_year\", \n",
    "                                \"v_weekday\", \"v_quarter\", \n",
    "                                \"v_hourly_segment\"], drop_first=True, prefix_sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conjunto de entrenamiento y prueba**\n",
    "\n",
    "Definimos el conjunto de entrenamiento con el 70% y el de prueba con el 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos los conjuntos\n",
    "X_train, X_test = train_test_split(df, test_size = 0.30, random_state = 413)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"v_residence_type\"].value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"v_residence_type\"].value_counts(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "d803cf1c0427546fb31cf9a0e3800339526bb22baaf4da162a3aac4c2cbd0647"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
